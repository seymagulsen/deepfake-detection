{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAAb0V3qBdzW",
        "outputId": "1ffc9092-1b0f-436f-83f2-ba8a55576804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ train_test_ids.json is completed successfully!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "train_test_ids = {\n",
        "    \"train\": {\n",
        "        \"original\": [f\"{i:03d}\" for i in range(120)],\n",
        "        \"deepfakes\": [f\"{i:03d}\" for i in range(40)],\n",
        "        \"face2face\": [f\"{i:03d}\" for i in range(30)],\n",
        "        \"faceshifter\": [f\"{i:03d}\" for i in range(30)],\n",
        "        \"faceswap\": [f\"{i:03d}\" for i in range(30)],\n",
        "    },\n",
        "    \"test\": {\n",
        "        \"original\": [f\"{i:03d}\" for i in range(120, 148)],\n",
        "        \"deepfakes\": [f\"{i:03d}\" for i in range(40, 47)],\n",
        "        \"face2face\": [f\"{i:03d}\" for i in range(30, 37)],\n",
        "        \"faceshifter\": [f\"{i:03d}\" for i in range(30, 37)],\n",
        "        \"faceswap\": [f\"{i:03d}\" for i in range(30, 37)],\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"train_test_ids.json\", \"w\") as f:\n",
        "    json.dump(train_test_ids, f)\n",
        "\n",
        "print(\"✅ train_test_ids.json is completed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pLoAY6QPBh1s",
        "outputId": "9ffd53a3-cae8-4ff5-faf8-f931e7202e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-daaa9714-9cca-41cc-967a-b240f9816ecf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-daaa9714-9cca-41cc-967a-b240f9816ecf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving faceforensics_download_v4.py to faceforensics_download_v4.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'faceforensics_download_v4.py': b'#!/usr/bin/env python\\n\"\"\" Downloads FaceForensics++ and Deep Fake Detection public data release\\nExample usage:\\n    see -h or https://github.com/ondyari/FaceForensics\\n\"\"\"\\n# -*- coding: utf-8 -*-\\nimport argparse\\nimport os\\nimport urllib\\nimport urllib.request\\nimport tempfile\\nimport time\\nimport sys\\nimport json\\nimport random\\nfrom tqdm import tqdm\\nfrom os.path import join\\n\\n\\n# URLs and filenames\\nFILELIST_URL = \\'misc/filelist.json\\'\\nDEEPFEAKES_DETECTION_URL = \\'misc/deepfake_detection_filenames.json\\'\\nDEEPFAKES_MODEL_NAMES = [\\'decoder_A.h5\\', \\'decoder_B.h5\\', \\'encoder.h5\\',]\\n\\n# Parameters\\nDATASETS = {\\n    \\'original_youtube_videos\\': \\'misc/downloaded_youtube_videos.zip\\',\\n    \\'original_youtube_videos_info\\': \\'misc/downloaded_youtube_videos_info.zip\\',\\n    \\'original\\': \\'original_sequences/youtube\\',\\n    \\'DeepFakeDetection_original\\': \\'original_sequences/actors\\',\\n    \\'Deepfakes\\': \\'manipulated_sequences/Deepfakes\\',\\n    \\'DeepFakeDetection\\': \\'manipulated_sequences/DeepFakeDetection\\',\\n    \\'Face2Face\\': \\'manipulated_sequences/Face2Face\\',\\n    \\'FaceShifter\\': \\'manipulated_sequences/FaceShifter\\',\\n    \\'FaceSwap\\': \\'manipulated_sequences/FaceSwap\\',\\n    \\'NeuralTextures\\': \\'manipulated_sequences/NeuralTextures\\'\\n    }\\nALL_DATASETS = [\\'original\\', \\'DeepFakeDetection_original\\', \\'Deepfakes\\',\\n                \\'DeepFakeDetection\\', \\'Face2Face\\', \\'FaceShifter\\', \\'FaceSwap\\',\\n                \\'NeuralTextures\\']\\nCOMPRESSION = [\\'raw\\', \\'c23\\', \\'c40\\']\\nTYPE = [\\'videos\\', \\'masks\\', \\'models\\']\\nSERVERS = [\\'EU\\', \\'EU2\\', \\'CA\\']\\n\\n\\ndef parse_args():\\n    parser = argparse.ArgumentParser(\\n        description=\\'Downloads FaceForensics v2 public data release.\\',\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n    parser.add_argument(\\'output_path\\', type=str, help=\\'Output directory.\\')\\n    parser.add_argument(\\'-d\\', \\'--dataset\\', type=str, default=\\'all\\',\\n                        help=\\'Which dataset to download, either pristine or \\'\\n                             \\'manipulated data or the downloaded youtube \\'\\n                             \\'videos.\\',\\n                        choices=list(DATASETS.keys()) + [\\'all\\']\\n                        )\\n    parser.add_argument(\\'-c\\', \\'--compression\\', type=str, default=\\'raw\\',\\n                        help=\\'Which compression degree. All videos \\'\\n                             \\'have been generated with h264 with a varying \\'\\n                             \\'codec. Raw (c0) videos are lossless compressed.\\',\\n                        choices=COMPRESSION\\n                        )\\n    parser.add_argument(\\'-t\\', \\'--type\\', type=str, default=\\'videos\\',\\n                        help=\\'Which file type, i.e. videos, masks, for our \\'\\n                             \\'manipulation methods, models, for Deepfakes.\\',\\n                        choices=TYPE\\n                        )\\n    parser.add_argument(\\'-n\\', \\'--num_videos\\', type=int, default=None,\\n                        help=\\'Select a number of videos number to \\'\\n                             \"download if you don\\'t want to download the full\"\\n                             \\' dataset.\\')\\n    parser.add_argument(\\'--server\\', type=str, default=\\'EU\\',\\n                        help=\\'Server to download the data from. If you \\'\\n                             \\'encounter a slow download speed, consider \\'\\n                             \\'changing the server.\\',\\n                        choices=SERVERS\\n                        )\\n    args = parser.parse_args()\\n\\n    # URLs\\n    server = args.server\\n    if server == \\'EU\\':\\n        server_url = \\'http://canis.vc.in.tum.de:8100/\\'\\n    elif server == \\'EU2\\':\\n        server_url = \\'http://kaldir.vc.in.tum.de/faceforensics/\\'\\n    elif server == \\'CA\\':\\n        server_url = \\'http://falas.cmpt.sfu.ca:8100/\\'\\n    else:\\n        raise Exception(\\'Wrong server name. Choices: {}\\'.format(str(SERVERS)))\\n    args.tos_url = server_url + \\'webpage/FaceForensics_TOS.pdf\\'\\n    args.base_url = server_url + \\'v3/\\'\\n    args.deepfakes_model_url = server_url + \\'v3/manipulated_sequences/\\' + \\\\\\n                               \\'Deepfakes/models/\\'\\n\\n    return args\\n\\n\\ndef download_files(filenames, base_url, output_path, report_progress=True):\\n    os.makedirs(output_path, exist_ok=True)\\n    if report_progress:\\n        filenames = tqdm(filenames)\\n    for filename in filenames:\\n        download_file(base_url + filename, join(output_path, filename))\\n\\n\\ndef reporthook(count, block_size, total_size):\\n    global start_time\\n    if count == 0:\\n        start_time = time.time()\\n        return\\n    duration = time.time() - start_time\\n    progress_size = int(count * block_size)\\n    speed = int(progress_size / (1024 * duration))\\n    percent = int(count * block_size * 100 / total_size)\\n    sys.stdout.write(\"\\\\rProgress: %d%%, %d MB, %d KB/s, %d seconds passed\" %\\n                     (percent, progress_size / (1024 * 1024), speed, duration))\\n    sys.stdout.flush()\\n\\n\\ndef download_file(url, out_file, report_progress=False):\\n    out_dir = os.path.dirname(out_file)\\n    if not os.path.isfile(out_file):\\n        fh, out_file_tmp = tempfile.mkstemp(dir=out_dir)\\n        f = os.fdopen(fh, \\'w\\')\\n        f.close()\\n        if report_progress:\\n            urllib.request.urlretrieve(url, out_file_tmp,\\n                                       reporthook=reporthook)\\n        else:\\n            urllib.request.urlretrieve(url, out_file_tmp)\\n        os.rename(out_file_tmp, out_file)\\n    else:\\n        tqdm.write(\\'WARNING: skipping download of existing file \\' + out_file)\\n\\n\\ndef main(args):\\n    # TOS\\n    print(\\'By pressing any key to continue you confirm that you have agreed \\'\\\\\\n          \\'to the FaceForensics terms of use as described at:\\')\\n    print(args.tos_url)\\n    print(\\'***\\')\\n    print(\\'Press any key to continue, or CTRL-C to exit.\\')\\n    _ = input(\\'\\')\\n\\n    # Extract arguments\\n    c_datasets = [args.dataset] if args.dataset != \\'all\\' else ALL_DATASETS\\n    c_type = args.type\\n    c_compression = args.compression\\n    num_videos = args.num_videos\\n    output_path = args.output_path\\n    os.makedirs(output_path, exist_ok=True)\\n\\n    # Check for special dataset cases\\n    for dataset in c_datasets:\\n        dataset_path = DATASETS[dataset]\\n        # Special cases\\n        if \\'original_youtube_videos\\' in dataset:\\n            # Here we download the original youtube videos zip file\\n            print(\\'Downloading original youtube videos.\\')\\n            if not \\'info\\' in dataset_path:\\n                print(\\'Please be patient, this may take a while (~40gb)\\')\\n                suffix = \\'\\'\\n            else:\\n            \\tsuffix = \\'info\\'\\n            download_file(args.base_url + \\'/\\' + dataset_path,\\n                          out_file=join(output_path,\\n                                        \\'downloaded_videos{}.zip\\'.format(\\n                                            suffix)),\\n                          report_progress=True)\\n            return\\n\\n        # Else: regular datasets\\n        print(\\'Downloading {} of dataset \"{}\"\\'.format(\\n            c_type, dataset_path\\n        ))\\n\\n        # Get filelists and video lenghts list from server\\n        if \\'DeepFakeDetection\\' in dataset_path or \\'actors\\' in dataset_path:\\n        \\tfilepaths = json.loads(urllib.request.urlopen(args.base_url + \\'/\\' +\\n                DEEPFEAKES_DETECTION_URL).read().decode(\"utf-8\"))\\n        \\tif \\'actors\\' in dataset_path:\\n        \\t\\tfilelist = filepaths[\\'actors\\']\\n        \\telse:\\n        \\t\\tfilelist = filepaths[\\'DeepFakesDetection\\']\\n        elif \\'original\\' in dataset_path:\\n            # Load filelist from server\\n            file_pairs = json.loads(urllib.request.urlopen(args.base_url + \\'/\\' +\\n                FILELIST_URL).read().decode(\"utf-8\"))\\n            filelist = []\\n            for pair in file_pairs:\\n            \\tfilelist += pair\\n        else:\\n            # Load filelist from server\\n            file_pairs = json.loads(urllib.request.urlopen(args.base_url + \\'/\\' +\\n                FILELIST_URL).read().decode(\"utf-8\"))\\n            # Get filelist\\n            filelist = []\\n            for pair in file_pairs:\\n                filelist.append(\\'_\\'.join(pair))\\n                if c_type != \\'models\\':\\n                    filelist.append(\\'_\\'.join(pair[::-1]))\\n        # Maybe limit number of videos for download\\n        if num_videos is not None and num_videos > 0:\\n        \\tprint(\\'Downloading the first {} videos\\'.format(num_videos))\\n        \\tfilelist = filelist[:num_videos]\\n\\n        # Server and local paths\\n        dataset_videos_url = args.base_url + \\'{}/{}/{}/\\'.format(\\n            dataset_path, c_compression, c_type)\\n        dataset_mask_url = args.base_url + \\'{}/{}/videos/\\'.format(\\n            dataset_path, \\'masks\\', c_type)\\n\\n        if c_type == \\'videos\\':\\n            dataset_output_path = join(output_path, dataset_path, c_compression,\\n                                       c_type)\\n            print(\\'Output path: {}\\'.format(dataset_output_path))\\n            filelist = [filename + \\'.mp4\\' for filename in filelist]\\n            download_files(filelist, dataset_videos_url, dataset_output_path)\\n        elif c_type == \\'masks\\':\\n            dataset_output_path = join(output_path, dataset_path, c_type,\\n                                       \\'videos\\')\\n            print(\\'Output path: {}\\'.format(dataset_output_path))\\n            if \\'original\\' in dataset:\\n                if args.dataset != \\'all\\':\\n                    print(\\'Only videos available for original data. Aborting.\\')\\n                    return\\n                else:\\n                    print(\\'Only videos available for original data. \\'\\n                          \\'Skipping original.\\\\n\\')\\n                    continue\\n            if \\'FaceShifter\\' in dataset:\\n                print(\\'Masks not available for FaceShifter. Aborting.\\')\\n                return\\n            filelist = [filename + \\'.mp4\\' for filename in filelist]\\n            download_files(filelist, dataset_mask_url, dataset_output_path)\\n\\n        # Else: models for deepfakes\\n        else:\\n            if dataset != \\'Deepfakes\\' and c_type == \\'models\\':\\n                print(\\'Models only available for Deepfakes. Aborting\\')\\n                return\\n            dataset_output_path = join(output_path, dataset_path, c_type)\\n            print(\\'Output path: {}\\'.format(dataset_output_path))\\n\\n            # Get Deepfakes models\\n            for folder in tqdm(filelist):\\n                folder_filelist = DEEPFAKES_MODEL_NAMES\\n\\n                # Folder paths\\n                folder_base_url = args.deepfakes_model_url + folder + \\'/\\'\\n                folder_dataset_output_path = join(dataset_output_path,\\n                                                  folder)\\n                download_files(folder_filelist, folder_base_url,\\n                               folder_dataset_output_path,\\n                               report_progress=False)   # already done\\n\\n\\nif __name__ == \"__main__\":\\n    args = parse_args()\\n    main(args)'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile faceforensics_download_v4.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqVhPGSSBh4U",
        "outputId": "530f0960-129c-43b3-c3ae-b8accfd0f71a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting faceforensics_download_v4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile faceforensics_download_v4.py\n",
        "import argparse\n",
        "import os\n",
        "import urllib.request\n",
        "import tempfile\n",
        "import time\n",
        "import sys\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from os.path import join\n",
        "import ast\n",
        "\n",
        "# Constants\n",
        "FILELIST_URL = 'misc/filelist.json'\n",
        "DEEPFEAKES_DETECTION_URL = 'misc/deepfake_detection_filenames.json'\n",
        "DEEPFAKES_MODEL_NAMES = ['decoder_A.h5', 'decoder_B.h5', 'encoder.h5']\n",
        "\n",
        "DATASETS = {\n",
        "    'original': 'original_sequences/youtube',\n",
        "    'Deepfakes': 'manipulated_sequences/Deepfakes',\n",
        "    'Face2Face': 'manipulated_sequences/Face2Face',\n",
        "    'FaceShifter': 'manipulated_sequences/FaceShifter',\n",
        "    'FaceSwap': 'manipulated_sequences/FaceSwap',\n",
        "}\n",
        "ALL_DATASETS = list(DATASETS.keys())\n",
        "COMPRESSION = ['raw', 'c23', 'c40']\n",
        "TYPE = ['videos']\n",
        "SERVERS = ['EU', 'EU2', 'CA']\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('output_path', type=str, help='Root directory to save data.')\n",
        "    parser.add_argument('-d', '--dataset', type=str, required=True, choices=ALL_DATASETS, help='Dataset name')\n",
        "    parser.add_argument('-c', '--compression', type=str, default='c23', choices=COMPRESSION)\n",
        "    parser.add_argument('-t', '--type', type=str, default='videos', choices=TYPE)\n",
        "    parser.add_argument('--server', type=str, default='EU', choices=SERVERS)\n",
        "    parser.add_argument('--video_ids', type=str, default=None, help='JSON file or list of video IDs')\n",
        "    parser.add_argument('--split', type=str, default='train', choices=['train', 'test'], help='Split name: train or test')\n",
        "    return parser.parse_args()\n",
        "\n",
        "def get_server_url(server):\n",
        "    if server == 'EU':\n",
        "        return 'http://canis.vc.in.tum.de:8100/'\n",
        "    elif server == 'EU2':\n",
        "        return 'http://kaldir.vc.in.tum.de/faceforensics/'\n",
        "    elif server == 'CA':\n",
        "        return 'http://falas.cmpt.sfu.ca:8100/'\n",
        "    else:\n",
        "        raise ValueError(\"Invalid server\")\n",
        "\n",
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "    else:\n",
        "        duration = time.time() - start_time\n",
        "        progress = int(count * block_size)\n",
        "        percent = int(progress * 100 / total_size)\n",
        "        sys.stdout.write(f\"\\r{percent}% ({progress // (1024*1024)} MB) downloaded\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "def download_file(url, dest):\n",
        "    os.makedirs(os.path.dirname(dest), exist_ok=True)\n",
        "    if not os.path.exists(dest):\n",
        "        tmp_file, tmp_path = tempfile.mkstemp(dir=os.path.dirname(dest))\n",
        "        os.close(tmp_file)\n",
        "        urllib.request.urlretrieve(url, tmp_path, reporthook=reporthook)\n",
        "        os.rename(tmp_path, dest)\n",
        "    else:\n",
        "        tqdm.write(f'Skipping already existing file: {dest}')\n",
        "\n",
        "def main(args):\n",
        "    base_url = get_server_url(args.server) + 'v3/'\n",
        "    dataset_key = args.dataset\n",
        "    dataset_path = DATASETS[dataset_key]\n",
        "    dataset_url = f\"{base_url}{dataset_path}/{args.compression}/{args.type}/\"\n",
        "\n",
        "    # Load filelist\n",
        "    file_pairs = json.loads(urllib.request.urlopen(base_url + FILELIST_URL).read().decode(\"utf-8\"))\n",
        "\n",
        "    if dataset_key == \"original\":\n",
        "        # Sadece birer dosya ismi var: \"000\", \"001\", ...\n",
        "        file_ids = set()\n",
        "        for pair in file_pairs:\n",
        "            file_ids.update(pair)  # hem 000 hem 001 gibi\n",
        "        filelist = list(file_ids)\n",
        "    else:\n",
        "        filelist = []\n",
        "        for pair in file_pairs:\n",
        "            filelist.append('_'.join(pair))\n",
        "            filelist.append('_'.join(pair[::-1]))\n",
        "\n",
        "\n",
        "    # Load video IDs\n",
        "    selected_ids = None\n",
        "    if args.video_ids:\n",
        "        if args.video_ids.endswith('.json'):\n",
        "            with open(args.video_ids, 'r') as f:\n",
        "                ids_json = json.load(f)\n",
        "            selected_ids = ids_json[args.split].get(dataset_key.lower(), [])\n",
        "        else:\n",
        "            selected_ids = ast.literal_eval(args.video_ids)\n",
        "\n",
        "    # Filter by IDs\n",
        "    if selected_ids:\n",
        "        filelist = [f for f in filelist if f.split('_')[-1] in selected_ids]\n",
        "        print(f\"✅ Filtered: {len(filelist)} files matched for {args.split} set\")\n",
        "\n",
        "    filelist = [f + \".mp4\" for f in filelist]\n",
        "\n",
        "    # Final output path\n",
        "    full_output_path = os.path.join(args.output_path, args.split, dataset_path, args.compression, args.type)\n",
        "    print(f\"📁 Downloading to: {full_output_path}\")\n",
        "\n",
        "    for f in tqdm(filelist):\n",
        "        download_file(dataset_url + f, os.path.join(full_output_path, f))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHahndSiBh7E",
        "outputId": "492dbc87-6eca-4e2e-cf74-0085d1383a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting faceforensics_download_v4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZuerggRDkR4",
        "outputId": "d4a4399f-8a09-4b93-da7a-003deb73e707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 faceforensics_download_v4.py /content/drive/MyDrive/faceforensics_data \\\n",
        "    -d Deepfakes -c c23 --video_ids train_test_ids.json --split train --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuoTMZZjBiH4",
        "outputId": "36754b19-c7aa-4720-ca53-08f79eb7c083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Filtered: 40 files matched for train set\n",
            "📁 Downloading to: /content/drive/MyDrive/faceforensics_data/train/manipulated_sequences/Deepfakes/c23/videos\n",
            "100% 40/40 [01:00<00:00,  1.50s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 faceforensics_download_v4.py /content/drive/MyDrive/faceforensics_data \\\n",
        "    -d Face2Face -c c23 --video_ids train_test_ids.json --split train --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUG8QZndEPkB",
        "outputId": "07af38c4-4760-449b-833c-1e00a66a3e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Filtered: 30 files matched for train set\n",
            "📁 Downloading to: /content/drive/MyDrive/faceforensics_data/train/manipulated_sequences/Face2Face/c23/videos\n",
            "100% 30/30 [00:37<00:00,  1.24s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 faceforensics_download_v4.py /content/drive/MyDrive/faceforensics_data \\\n",
        "    -d FaceShifter -c c23 --video_ids train_test_ids.json --split train --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy-cqpf3EPmo",
        "outputId": "f9fe0a27-9e6b-46c4-cc22-9fae5a7f76cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Filtered: 30 files matched for train set\n",
            "📁 Downloading to: /content/drive/MyDrive/faceforensics_data/train/manipulated_sequences/FaceShifter/c23/videos\n",
            "100% 30/30 [00:37<00:00,  1.26s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 faceforensics_download_v4.py /content/drive/MyDrive/faceforensics_data \\\n",
        "    -d FaceSwap -c c23 --video_ids train_test_ids.json --split train --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ewj8m94EPpR",
        "outputId": "292c31e3-3a36-498f-ff32-baa0e92c51a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Filtered: 30 files matched for train set\n",
            "📁 Downloading to: /content/drive/MyDrive/faceforensics_data/train/manipulated_sequences/FaceSwap/c23/videos\n",
            "100% 30/30 [00:38<00:00,  1.28s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 faceforensics_download_v4.py /content/drive/MyDrive/faceforensics_data \\\n",
        "    -d original -c c23 --video_ids train_test_ids.json --split train --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ftLLyYhEPsA",
        "outputId": "1531a01a-96b3-4b3b-d9b3-2ffe81a1e9ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Filtered: 120 files matched for train set\n",
            "📁 Downloading to: /content/drive/MyDrive/faceforensics_data/train/original_sequences/youtube/c23/videos\n",
            "100% 120/120 [02:32<00:00,  1.27s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 faceforensics_download_v4.py /content/drive/MyDrive/faceforensics_data \\\n",
        "    -d Deepfakes -c c23 --video_ids train_test_ids.json --split test --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlrC0pVDGygc",
        "outputId": "b38a23a6-2159-4444-a3a0-e96943c90852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Filtered: 7 files matched for test set\n",
            "📁 Downloading to: /content/drive/MyDrive/faceforensics_data/test/manipulated_sequences/Deepfakes/c23/videos\n",
            "100% 7/7 [00:11<00:00,  1.60s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 faceforensics_download_v4.py /content/drive/MyDrive/faceforensics_data \\\n",
        "    -d Face2Face -c c23 --video_ids train_test_ids.json --split test --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tijyp9riGyrE",
        "outputId": "e9d3499f-fb05-4f93-b347-8c39a7b94299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Filtered: 7 files matched for test set\n",
            "📁 Downloading to: /content/drive/MyDrive/faceforensics_data/test/manipulated_sequences/Face2Face/c23/videos\n",
            "100% 7/7 [00:08<00:00,  1.28s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 faceforensics_download_v4.py /content/drive/MyDrive/faceforensics_data \\\n",
        "    -d FaceShifter -c c23 --video_ids train_test_ids.json --split test --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etbYilcyGyts",
        "outputId": "298ec3bd-444f-405c-93d2-b3341e61b49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Filtered: 7 files matched for test set\n",
            "📁 Downloading to: /content/drive/MyDrive/faceforensics_data/test/manipulated_sequences/FaceShifter/c23/videos\n",
            "100% 7/7 [00:09<00:00,  1.33s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 faceforensics_download_v4.py /content/drive/MyDrive/faceforensics_data \\\n",
        "    -d FaceSwap -c c23 --video_ids train_test_ids.json --split test --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAD2udJnGyzc",
        "outputId": "ed7521a4-959f-44c9-ad1f-7bdecc3f1dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Filtered: 7 files matched for test set\n",
            "📁 Downloading to: /content/drive/MyDrive/faceforensics_data/test/manipulated_sequences/FaceSwap/c23/videos\n",
            "100% 7/7 [00:08<00:00,  1.25s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 faceforensics_download_v4.py /content/drive/MyDrive/faceforensics_data \\\n",
        "    -d original -c c23 --video_ids train_test_ids.json --split test --server EU2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8CBn85WGy18",
        "outputId": "df69da8c-d498-4fd8-bb9f-9d47cfc327e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Filtered: 28 files matched for test set\n",
            "📁 Downloading to: /content/drive/MyDrive/faceforensics_data/test/original_sequences/youtube/c23/videos\n",
            "100% 28/28 [00:35<00:00,  1.25s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# SETTINGS\n",
        "every_n = 5  # Extract every 5th frame\n",
        "\n",
        "# 📁 Google Drive paths\n",
        "video_root = Path(\"/content/drive/MyDrive/faceforensics_data/train\")\n",
        "frame_root = Path(\"/content/drive/MyDrive/face_forensics_frames/train\")\n",
        "os.makedirs(frame_root, exist_ok=True)\n",
        "\n",
        "# Mapping: dataset folder → class name\n",
        "dataset_map = {\n",
        "    \"original_sequences/youtube\": \"original\",\n",
        "    \"manipulated_sequences/Deepfakes\": \"deepfakes\",\n",
        "    \"manipulated_sequences/Face2Face\": \"face2face\",\n",
        "    \"manipulated_sequences/FaceShifter\": \"faceshifter\",\n",
        "    \"manipulated_sequences/FaceSwap\": \"faceswap\"\n",
        "}\n",
        "\n",
        "# Function to extract frames from video\n",
        "def extract_frames(video_path, output_dir, every_n=5):\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    frame_idx = 0\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    while success:\n",
        "        if frame_idx % every_n == 0:\n",
        "            out_path = output_dir / f\"{video_path.stem}_frame{frame_idx}.jpg\"\n",
        "            cv2.imwrite(str(out_path), frame)\n",
        "        success, frame = cap.read()\n",
        "        frame_idx += 1\n",
        "    cap.release()\n",
        "\n",
        "# Process each dataset\n",
        "for subfolder, label in dataset_map.items():\n",
        "    video_dir = video_root / subfolder / \"c23\" / \"videos\"\n",
        "    output_dir = frame_root / label\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    video_files = list(video_dir.glob(\"*.mp4\"))\n",
        "    print(f\"📦 {label}: Found {len(video_files)} videos\")\n",
        "\n",
        "    for video_path in tqdm(video_files, desc=f\"Extracting {label}\"):\n",
        "        extract_frames(video_path, output_dir, every_n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAT_O4xJEPuJ",
        "outputId": "c1c65851-559f-407a-bc34-1b4988b24d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 original: Found 120 videos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting original: 100%|██████████| 120/120 [13:33<00:00,  6.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 deepfakes: Found 40 videos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting deepfakes: 100%|██████████| 40/40 [04:24<00:00,  6.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 face2face: Found 30 videos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting face2face: 100%|██████████| 30/30 [02:55<00:00,  5.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 faceshifter: Found 30 videos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting faceshifter: 100%|██████████| 30/30 [03:12<00:00,  6.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 faceswap: Found 30 videos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting faceswap: 100%|██████████| 30/30 [02:20<00:00,  4.68s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# SOURCE base directory (on Google Drive)\n",
        "base_dir = Path(\"/content/drive/MyDrive/face_forensics_frames/train\")\n",
        "\n",
        "# Target directory where all fakes will be merged\n",
        "fake_dir = base_dir / \"fake_sequences\"\n",
        "os.makedirs(fake_dir, exist_ok=True)\n",
        "\n",
        "# List of fake classes to merge\n",
        "fake_classes = [\"deepfakes\", \"face2face\", \"faceshifter\", \"faceswap\"]\n",
        "\n",
        "# Copy files from each class into fake_sequences/\n",
        "for cls in fake_classes:\n",
        "    src_dir = base_dir / cls\n",
        "    image_files = list(src_dir.glob(\"*.jpg\"))\n",
        "    print(f\"📦 Merging {len(image_files)} images from '{cls}'\")\n",
        "\n",
        "    for img_path in tqdm(image_files, desc=f\"Merging {cls}\"):\n",
        "        # Optional: prefix class name to avoid name collision\n",
        "        dest_filename = f\"{cls}_{img_path.name}\"\n",
        "        shutil.copy(img_path, fake_dir / dest_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taONzycSEfI4",
        "outputId": "710ed578-4bc2-4b82-8218-4bac1b88929d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Merging 3920 images from 'deepfakes'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Merging deepfakes: 100%|██████████| 3920/3920 [02:07<00:00, 30.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Merging 2938 images from 'face2face'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Merging face2face: 100%|██████████| 2938/2938 [01:39<00:00, 29.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Merging 3049 images from 'faceshifter'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Merging faceshifter: 100%|██████████| 3049/3049 [01:49<00:00, 27.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Merging 2305 images from 'faceswap'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Merging faceswap: 100%|██████████| 2305/2305 [01:17<00:00, 29.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# 📂 Base path definer\n",
        "base_path = Path(\"/content/drive/MyDrive/face_forensics_frames/train\")\n",
        "\n",
        "counts = {}\n",
        "\n",
        "for class_folder in sorted(base_path.iterdir()):\n",
        "    if class_folder.is_dir():\n",
        "        jpg_files = list(class_folder.glob(\"*.jpg\"))\n",
        "        counts[class_folder.name] = len(jpg_files)\n",
        "\n",
        "# ✨ Results\n",
        "total = 0\n",
        "print(\"📊 Frame numbers:\\n\")\n",
        "for name, count in counts.items():\n",
        "    print(f\"{name:<15}: {count:6}\")\n",
        "    total += count\n",
        "\n",
        "print(\"\\n🧮 Total frame numbers:\", total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yioyaKN2EfLU",
        "outputId": "4ab5aaee-9eda-48d2-ec57-dfaa095cb7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Frame numbers:\n",
            "\n",
            "deepfakes      :   3920\n",
            "face2face      :   2938\n",
            "faceshifter    :   3049\n",
            "faceswap       :   2305\n",
            "fake_sequences :  12212\n",
            "original       :  12068\n",
            "\n",
            "🧮 Total frame numbers: 36492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4wIDbdzcEfN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RnD3M6b5EfP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MRLlb8X6EP0A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}